{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Filter Confusing Questions for Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a CSV-file with short documents (a few text paragraphs each), we want to generate confusing questions about these documents. A question is _confusing_ if it contains a false premise and therefore has no good answer. We want to use confusing questions in order to improve LLM ability to detect false premises and point them out to the user who asked the question, rather than play along and create even more confusion by trying to answer an unanswerable question.\n",
    "\n",
    "Our confusing questions must be complex enough so that the \"unprepared\" LLM plays along and attempts to answer them, but simple enough so that the same LLM detects the false premise when specifically asked to search for it. In other words, each question must satisfy the following constraints:\n",
    "\n",
    "* Be based on the contents of the document\n",
    "* Start off with a false premise or assumption, not just ask if the assumption is true\n",
    "* Have no good answer (positive or negative) other than pointing out the false assumption to the user\n",
    "* The \"unprepared\" LLM must answer the question _AS IF_ it is answerable (creating even more confusion)\n",
    "* The same LLM must detect the false premise when specifically asked to search for it\n",
    "* Moreover, the same LLM must be able to detect that its original answer has failed to point out the false premise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses two CSV formats: one for the table with documents, and one for the table with individual questions and responses. The input CSV table is expected to have 3 columns: document ID, document source (e.g. URL), and the document itself. We want to keep track of document sources to avoid licensing issues. (Maybe we should add more columns for timestamp, for the license, etc.) Once the list of questions is generated, we add them as extra columns for easy review. Below is the schema of the table with documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_csv_schema = {\n",
    "    \"doc_id\" : \"doc_id\",           # Column with a unique document ID\n",
    "    \"source\" : \"source\",           # Column with document source (e.g. URL)\n",
    "    \"document\" : \"document\",       # Column with the text of the original document\n",
    "    \"LLM_q\" : \"LLM_q\",             # Column with name of the LLM that generated confusing questions\n",
    "    \"orig_qs\" : \"orig_questions\",  # Column with the original (non-confusing) questions\n",
    "    \"conf_qs\" : \"conf_questions\"   # Column with the modified (confusing) questions\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `LLM_q` refers to the LLM used to _generate_ the confusing questions, which does not have to be the same LLM as the one (`LLM_r`) used to respond to the questions and test for the false premises. In fact, it is better if `LLM_q` is stronger than `LLM_r`.\n",
    "\n",
    "Below is the schema of the CSV table used to store individual questions (one question per row), responses, false premises, and their detection information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrc_csv_schema = {\n",
    "    \"doc_id\" : \"doc_id\",           # Column with document ID, same as the other table\n",
    "    \"q_id\" : \"q_id\",               # Column with question ID (for this document)\n",
    "    \"is_conf\" : \"is_confusing\",    # Column with \"yes\" or \"no\" indicating if the question is confusing\n",
    "    \"question\" : \"question\",       # Column with the question (either original or confusing)\n",
    "    \"LLM_r\" : \"LLM_r\",             # Column with name of the LLM that generated the responses\n",
    "    \"response\" : \"response\",       # Column with response generated given the document and the question\n",
    "    \"confusion\" : \"confusion\",     # Column with LLM-found confusion in the question (or \"none\")\n",
    "    \"defusion\" : \"defusion\",       # Column with LLM's reply on whether its own response detected the confusion\n",
    "    \"is_defused\" : \"is_defused\"    # Column with \"yes\" or \"no\" as LLM checks if response detected the confusion\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify the two LLMs, `LLM_q` and `LLM_r`, by short names defined as keys in `llmlib.py`. We also specify the input CSV table with documents and the output CSV tables: one with a row per document and one with a row per question/response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptlib import read_prompts\n",
    "from datagen import *\n",
    "import os\n",
    "\n",
    "llm_q = \"llama3-8B-in\" # \"gpt-3.5\" # \"gpt-4o\"  # LLM for generating questions\n",
    "llm_r = \"llama3-8B-in\" # \"gpt-3.5\"  # LLM for generating responses\n",
    "\n",
    "num_q = 10  # Number of questions generated per document\n",
    "\n",
    "prompts_folder = \"prompts\"\n",
    "data_folder = \"experiments/2024-07-18-llama3\"\n",
    "\n",
    "input_doc_path  = os.path.join(data_folder, \"docs_in.csv\")\n",
    "output_doc_path = os.path.join(data_folder, \"docs_out.csv\")\n",
    "output_qrc_path = os.path.join(data_folder, \"qrc_out.csv\")\n",
    "filter_qrc_path = os.path.join(data_folder, \"qrc_filter.csv\")\n",
    "\n",
    "read_prompts(prompts_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:**  For each document, ask `LLM_q` to write `num_q` questions answered in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the input document table from CSV file:\n",
      "    experiments/2024-07-18-llama3/docs_in.csv\n",
      "    Rows: 20,  Cols: 3\n",
      "    Index(['doc_id', 'source', 'document'], dtype='object')\n",
      "Generate 10 questions for each document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:16<00:00,  6.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the table with questions to CSV file:\n",
      "    experiments/2024-07-18-llama3/docs_1.csv\n",
      "    Rows: 20,  Cols: 5\n",
      "    Index(['doc_id', 'source', 'document', 'LLM_q', 'orig_questions'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "doc_path_1 = os.path.join(data_folder, \"docs_1.csv\")\n",
    "generate_questions_for_documents(llm_q, num_q, doc_csv_schema, input_doc_path, doc_path_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:**  Ask `LLM_q` to modify each question in the list so that it makes a false assumption. This one is tricky and requires few-shot examples to get right. Any improvements are welcome! Since `LLM_q` may be a strong and expensive LLM, we call it once for a list of questions, not for each individual question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the document-and-questions table from CSV file:\n",
      "    experiments/2024-07-18-llama3/docs_1.csv\n",
      "    Rows: 20,  Cols: 5\n",
      "    Index(['doc_id', 'source', 'document', 'LLM_q', 'orig_questions'], dtype='object')\n",
      "Modify each question by adding confusing (false) assumptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:02<00:00,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the table with confusing questions to CSV file:\n",
      "    experiments/2024-07-18-llama3/docs_out.csv\n",
      "    Rows: 20,  Cols: 6\n",
      "    Index(['doc_id', 'source', 'document', 'LLM_q', 'orig_questions',\n",
      "       'conf_questions'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_path_1 = os.path.join(data_folder, \"docs_1.csv\")\n",
    "infuse_questions_with_false_assumptions(doc_csv_schema, doc_path_1, output_doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:**  Switching to `LLM_r` now. Give the LLM the document and the question and record the LLM's response as in RAG setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the document-and-questions table from CSV file:\n",
      "    experiments/2024-07-18-llama3/docs_out.csv\n",
      "    Rows: 20,  Cols: 6\n",
      "    Index(['doc_id', 'source', 'document', 'LLM_q', 'orig_questions',\n",
      "       'conf_questions'],\n",
      "      dtype='object')\n",
      "Generate RAG response for each question, both original and confusing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [09:10<00:00, 27.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the question-response table to CSV file:\n",
      "    experiments/2024-07-18-llama3/qrc_1.csv\n",
      "    Rows: 400,  Cols: 6\n",
      "    Index(['doc_id', 'q_id', 'is_confusing', 'question', 'LLM_r', 'response'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qrc_path_1 = os.path.join(data_folder, \"qrc_1.csv\")\n",
    "generate_RAG_responses(llm_r, doc_csv_schema, output_doc_path, qrc_csv_schema, qrc_path_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:**  Ask `LLM_r` to find the false assumption in each question (including the original \"clean\" questions, used as control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the document table from CSV file:\n",
      "    experiments/2024-07-18-llama3/docs_out.csv\n",
      "    Rows: 20,  Cols: 6\n",
      "    Index(['doc_id', 'source', 'document', 'LLM_q', 'orig_questions',\n",
      "       'conf_questions'],\n",
      "      dtype='object')\n",
      "Create a dictionary of indexed documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 234.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the question-response table from CSV file:\n",
      "    experiments/2024-07-18-llama3/qrc_1.csv\n",
      "    Rows: 400,  Cols: 6\n",
      "    Index(['doc_id', 'q_id', 'is_confusing', 'question', 'LLM_r', 'response'], dtype='object')\n",
      "Ask LLM to find a false assumption in each question, or say 'none'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [10:31<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the question-response table to CSV file:\n",
      "    experiments/2024-07-18-llama3/qrc_2.csv\n",
      "    Rows: 400,  Cols: 7\n",
      "    Index(['doc_id', 'q_id', 'is_confusing', 'question', 'LLM_r', 'response',\n",
      "       'confusion'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qrc_path_2 = os.path.join(data_folder, \"qrc_2.csv\")\n",
    "find_false_assumptions_in_questions(doc_csv_schema, output_doc_path, qrc_csv_schema, qrc_path_1, qrc_path_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5:**  Ask the LLM to check if its initial response pointed out the false assumption. We are interested in the questions that (a) contained a false assumption, (b) LLM's response missed it, (c) now the LLM can see that it missed the false assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the document table from CSV file:\n",
      "    experiments/2024-07-18-llama3/docs_out.csv\n",
      "    Rows: 20,  Cols: 6\n",
      "    Index(['doc_id', 'source', 'document', 'LLM_q', 'orig_questions',\n",
      "       'conf_questions'],\n",
      "      dtype='object')\n",
      "Create a dictionary of indexed documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1242.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the question-response table from CSV file:\n",
      "    experiments/2024-07-18-llama3/qrc_2.csv\n",
      "    Rows: 400,  Cols: 7\n",
      "    Index(['doc_id', 'q_id', 'is_confusing', 'question', 'LLM_r', 'response',\n",
      "       'confusion'],\n",
      "      dtype='object')\n",
      "Ask LLM to check if its own response defused the confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [06:54<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the question-response table to CSV file:\n",
      "    experiments/2024-07-18-llama3/qrc_out.csv\n",
      "    Rows: 400,  Cols: 9\n",
      "    Index(['doc_id', 'q_id', 'is_confusing', 'question', 'LLM_r', 'response',\n",
      "       'confusion', 'defusion', 'is_defused'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_if_response_defused_confusion(doc_csv_schema, output_doc_path, qrc_csv_schema, qrc_path_2, output_qrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6:**  Count the number of both original and confusing quesions where confusion was detected, as performance metrics (true and false positives and negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the question-response table from CSV file:\n",
      "    experiments/2024-07-18-llama3/qrc_out.csv\n",
      "    Rows: 400,  Cols: 9\n",
      "    Index(['doc_id', 'q_id', 'is_confusing', 'question', 'LLM_r', 'response',\n",
      "       'confusion', 'defusion', 'is_defused'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 6491.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (non-confusing) questions:\n",
      "    Total questions = 200\n",
      "    With confusion detected = 122\n",
      "    With confusion detected and defused = 0\n",
      "Confusing questions:\n",
      "    Total questions = 200\n",
      "    With confusion detected = 74\n",
      "    With confusion detected and defused = 4\n",
      "    With confusion detected, but not defused = 70\n",
      "Write the filtered question-response table to CSV file:\n",
      "    experiments/2024-07-18-llama3/qrc_filter.csv\n",
      "    Rows: 70,  Cols: 9\n",
      "    Index(['doc_id', 'q_id', 'is_confusing', 'question', 'LLM_r', 'response',\n",
      "       'confusion', 'defusion', 'is_defused'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filter_undefused_confusions_and_compute_metrics(qrc_csv_schema, output_qrc_path, filter_qrc_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
